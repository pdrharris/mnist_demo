{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In machine learning with images, the place that everyone starts is with MNIST. \n",
    "\n",
    "The MNIST dataset is a collection of 70,000 images of handwritten digits from 0 to 9. The images are 28 by 28 pixels in size and are in grayscale. The dataset was created by combining two datasets from the National Institute of Standards and Technology (NIST). It was put together by Yann LeCun, Corinna Cortes, and Christopher J.C. Burges in 1994.\n",
    "\n",
    "Yann LeCun also developed the convolutional neural network. Back in 2012 (an age ago in machine learning!), this network enabled him to achieve state of the art results with the famous ImageNet dataset - improving the previous year's results by over 10%. While his work has been built on and developed, the convolutional network that we will look at below has been the backbone of all computer vision (any work that involves images) ever since."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guide to this notebook\n",
    "This notebook contains all of the code that you need to train an PyTorch model on the MNIST dataset. Assuming that you are seeing this in a Jupyter notebook, it needs three other things to train the model:\n",
    "- The key Python libraries installed (as listed below).\n",
    "- The `mnistdataset.py` file, which should already by here as part of the same zip file.\n",
    "- The data, which again should already be here in the same zip file under the _data_ directory.\n",
    "\n",
    "To run each cell, press the _play_ button beside each cell or select the cell and press _shift-enter_.\n",
    "\n",
    "**Note:** the cells with code in them need to be run in the order that they come in this notebook.\n",
    "\n",
    "My hope is that you'll play with the notebook, create new cells, tweak things and explore and, as you do so, grow in your understanding. Indeed, it would be great if this notebook became a template for you to do some experiments of your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "Start with imports from installed Python libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key libraries to install:\n",
    "- pytorch\n",
    "- pandas\n",
    "- numpy\n",
    "- matplotlib\n",
    "- sklearn\n",
    "\n",
    "Sandboxed Python install:\n",
    "- Anaconda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's also import our dataset from the mnist_dataset.py file that we wrote earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist_dataset import MnistDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a python hint - by typing in a function and putting a question mark after it, you get the help documentation for that function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m \u001b[0mMnistDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "A custom PyTorch dataset class for loading the MNIST dataset.\n",
      "\n",
      "The dataset expects to find a directory under data_root called 'train' or 'test'.\n",
      "This will contain the train or the test data.\n",
      "\n",
      "The dataset also expects to find a file called 'train.csv' or 'test.csv' in the\n",
      "specified data directory.\n",
      "    The 'train.csv' or 'test.csv' file should contain a column called 'png' that contains\n",
      "    the name of each image file in the dataset.\n",
      "    The 'train.csv' or 'test.csv' should also contain a column called 'label'\n",
      "    that contains the correct label for each image in the dataset.\n",
      "\n",
      "See mnist_dataset.ipynb for more details on this class and the csv files.\n",
      "\n",
      "Args:\n",
      "    data_root (Path or str): The root directory containing the dataset.\n",
      "    mode (str): The mode of the dataset (either 'train' or 'test').\n",
      "\n",
      "Attributes:\n",
      "    data_path (Path): The path to the directory for this dataset (the train or the test dataset).\n",
      "    image_info (pd.DataFrame): A Pandas DataFrame containing information about the images in the dataset.\n",
      "\n",
      "Methods:\n",
      "    __len__: Returns the number of images in the dataset.\n",
      "    __getitem__: Returns the image and label data for the specified index.\n",
      "\n",
      "Examples:\n",
      "    train_dataset = MnistDataset('/data/mnist_data', 'train')\n",
      "    len(train_dataset)\n",
      "        [returns the length of the dataset]\n",
      "    train_dataset[0]\n",
      "        [returns the image and label data for the first image in the dataset]\n",
      "\u001b[1;31mInit docstring:\u001b[0m\n",
      "Initializes a new instance of the MnistDataset class.\n",
      "\n",
      "Args:\n",
      "    data_root (Path or str): The root directory containing the dataset.\n",
      "    mode (Path): The mode of the dataset (either 'train' or 'test').\n",
      "\u001b[1;31mFile:\u001b[0m           c:\\users\\hape8001\\onedrive - the nielsen company\\code\\mnist_demo\\notebooks\\mnist_dataset.py\n",
      "\u001b[1;31mType:\u001b[0m           type\n",
      "\u001b[1;31mSubclasses:\u001b[0m     \n"
     ]
    }
   ],
   "source": [
    "MnistDataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPU_AVAILABLE = torch.cuda.is_available()\n",
    "GPU_AVAILABLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load up the datasets and examine a sample datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = MnistDataset(data_root, 'test')\n",
    "train_ds = MnistDataset(data_root, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_datapoint = train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x', 'Y'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_datapoint.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a simple 2D convolutional model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a picture of the network that we're going to use. (Credit: https://codetolight.wordpress.com/2017/11/29/getting-started-with-pytorch-for-deep-learning-part-3-neural-network-basics/)\n",
    "\n",
    "Below that is the code and then a line-by-line explanation of the code.\n",
    "\n",
    "![MNIST_2D_CONV.png](MNIST_2D_CONV.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MnistModel, self).__init__()\n",
    "        # input is 28x28\n",
    "        # padding=2 for same padding\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5, padding=2)\n",
    "        # feature map size is 14 * 14 by pooling\n",
    "        # padding=2 for same padding\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, padding=2)\n",
    "        # feature map size is 7 * 7 by pooling\n",
    "\n",
    "        self.linear_layer1 = nn.Linear(64 * 7 * 7, 100)\n",
    "        self.linear_layer2 = nn.Linear(100, 10)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        # Note: for clarity, we're using softmax, not log_softmax here\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Layer 1:\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        # Layer 2 (same thing but more concise code and with conv2 not conv1):\n",
    "        x = self.maxpool(self.relu(self.conv2(x)))\n",
    "\n",
    "        # Now convert from a 7x7 image to a flat 1D vector\n",
    "        x = x.view(-1, 64 * 7 * 7)   # reshape Variable\n",
    "\n",
    "        # Layer 3 (the linear layer):\n",
    "        x = self.linear_layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear_layer2(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a PyTorch model class called `MnistModel` for classifying MNIST images. The model is a type of convolutional neural network (CNN), which is commonly used for image classification tasks.\n",
    "\n",
    "Here's a breakdown of what each part of the code does:\n",
    "\n",
    "- `class MnistModel(nn.Module):` This line defines the `MnistModel` class, which inherits from the `nn.Module` class. In PyTorch, all neural network models should inherit from `nn.Module`.\n",
    "### The `__init__` method\n",
    "\n",
    "- `def __init__(self):` This method is the initializer of the class. It's where we define the layers of the neural network.\n",
    "\n",
    "- `super(MnistModel, self).__init__():` This line calls the initializer of the parent class (`nn.Module`). This is necessary to ensure that the `MnistModel` class is properly initialized.\n",
    "\n",
    "- `self.conv1 = nn.Conv2d(1, 32, 5, padding=2):` This line defines a 2D convolutional layer with 1 input channel, 32 output channels, a kernel size of 5, and padding of 2. Convolutional layers are used to extract features from the input image.\n",
    "\n",
    "- `self.conv2 = nn.Conv2d(32, 64, 5, padding=2):` This line defines another 2D convolutional layer, this time with 32 input channels and 64 output channels.\n",
    "\n",
    "- `self.linear_layer1 = nn.Linear(64 * 7 * 7, 1024):` This line defines a linear (or fully connected) layer that takes a flat input of size 64 * 7 * 7 and outputs a vector of size 1024.\n",
    "\n",
    "- `self.linear_layer2 = nn.Linear(1024, 10):` This line defines another linear layer that takes an input of size 1024 and outputs a vector of size 10.\n",
    "\n",
    "- `self.relu = nn.ReLU():` This line defines a ReLU (Rectified Linear Unit) activation function, which is used to introduce non-linearity into the model.\n",
    "\n",
    "- `self.dropout = nn.Dropout(0.5):` This line defines a dropout layer with a dropout rate of 0.5. Dropout is a regularization technique that helps to prevent overfitting.\n",
    "\n",
    "- `self.maxpool = nn.MaxPool2d(2):` This line defines a 2D max pooling layer with a kernel size of 2. Max pooling is used to downsample the feature maps.\n",
    "\n",
    "- `self.softmax = nn.Softmax(dim=1):` This line defines a softmax function, which is used to convert the output of the model into a probability distribution over the 10 classes.\n",
    "\n",
    "### The `forward` method\n",
    "- `def forward(self, x):` This method defines the forward pass of the model, i.e., how the input data `x` is processed by the layers of the network to produce the output.\n",
    "\n",
    "- `x = self.conv1(x)`, `x = self.relu(x)`, `x = self.maxpool(x)`: These lines pass the input data through the first convolutional layer, apply the ReLU activation function, and then apply max pooling.\n",
    "\n",
    "- `x = self.maxpool(self.relu(self.conv2(x)))`: This line does the same thing for the second convolutional layer, but in a more concise way.\n",
    "\n",
    "- `x = x.view(-1, 64 * 7 * 7)`: This line reshapes the 2D feature maps into a 1D vector, which is necessary for passing the data through the linear layers.\n",
    "\n",
    "- `x = self.linear_layer1(x)`, `x = self.relu(x)`, `x = self.dropout(x)`, `x = self.linear_layer2(x)`: These lines pass the data through the first linear layer, apply the ReLU activation function, apply dropout, and then pass the data through the second linear layer.\n",
    "\n",
    "- `return self.softmax(x)`: This line applies the softmax function to the output of the model, converting it into a probability distribution over the 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's initialise the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MnistModel(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (linear_layer1): Linear(in_features=3136, out_features=100, bias=True)\n",
       "  (linear_layer2): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MnistModel()\n",
    "model = model.cuda() if GPU_AVAILABLE else model\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step - we're just going to put a single datapoint through model.\n",
    "\n",
    "Things to note:\n",
    "- To get a single datapoint into the model, we need to do the slightly weird ``unsqueeze(0)`` bit. Don't worry about that. It's just compensating for the fact that the model is expecting multiple datapoints and we're only giving it one.\n",
    "- The model outputs something like confidence scores. However, to make the maths easier and faster, we usually output the log of the confidences scores. One day, when you're bored, you can look it up and see why it makes the maths so much easier and faster. So, in the real world, I'd replace `nn.Softmax` with `nn.LogSoftmax`. Further down, the loss function (`CrossEntropyLoss`) would need to be replaced with `NLLLoss`. But that's for another day!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1097, 0.0986, 0.0969, 0.0899, 0.0925, 0.1006, 0.1034, 0.0986, 0.1041,\n",
       "         0.1057]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = sample_datapoint['x'], sample_datapoint['Y']\n",
    "if GPU_AVAILABLE:\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "logits = model(x.unsqueeze(0))\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output above:\n",
    "- the first figure is the model's confidence that this digit is a zero\n",
    "- the second figure is the model's confidence that this digit is a 1\n",
    "- and so on\n",
    "\n",
    "We call these confidence scores \"logits\".\n",
    "\n",
    "So, to get the model's prediction, we need to find which confidence score is the biggest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1097], device='cuda:0', grad_fn=<MaxBackward0>),\n",
       " tensor([0], device='cuda:0'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence_value, prediction = torch.max(logits, dim=1)\n",
    "confidence_value, prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data into the model in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 1, 28, 28]), torch.Size([100]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_dl))\n",
    "batch['x'].shape, batch['Y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 7, 8, 0, 5, 5, 2, 2, 2, 6, 2, 1, 0, 3, 9])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the correct for the first 15 images in the batch:\n",
    "batch['Y'][:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two more things that we need to train the model.\n",
    "- The loss function compares the model's prediction with the correct answer and returns a score.\n",
    "    - The absolute standard loss function is call Cross Entropy Loss and it's what we're using here.\n",
    "- The Optimiser tells the model how much it should change each of its weights, based on the output of the loss function.\n",
    "    - My standard is to try lots of things and almost always come back to using an Adam optimiser!\n",
    "    - lr is the learning rate and it says how much the model should update itself based on the loss.\n",
    "    - A higher learning rate means that that the model can learn faster but it also makes it less accurate and sometimes means that it won't train at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function trains a single batch of data.\n",
    "\n",
    "There are lots of inline comments (the text following each #) to explain what it's doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(model, batch, optimizer, loss_function):\n",
    "    \"\"\"\n",
    "    Trains the model on a single batch of data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to train.\n",
    "        batch (dict): A dictionary containing the input data and labels for a single batch.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer to use for training.\n",
    "        loss_function (nn.Module): The loss function to use for training.\n",
    "\n",
    "    Returns:\n",
    "        float: The loss value for the batch.\n",
    "    \"\"\"\n",
    "    # Get the input data and labels from the batch\n",
    "    x, y = batch['x'], batch['Y']\n",
    "    # Move the data to the correct device, that is, onto the GPU if we have one available.\n",
    "    if GPU_AVAILABLE:\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "\n",
    "    # This next line is boilerplate PyTorch. It tells PyTorch to reset all gradients.\n",
    "    # You'll use this line as-is most of the time.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute the output of the model using the input data.\n",
    "    output = model(x)\n",
    "    # Calculate the loss using the output of the model and the correct labels.\n",
    "    loss = loss_function(output, y)\n",
    "\n",
    "    # This next lines are also boilerplate PyTorch.You'll use them as-is most of the time.\n",
    "    # loss.backward() tells PyTorch to compute gradients based on the loss function.\n",
    "    loss.backward()\n",
    "    # optimizer.step() tells PyTorch to update the model parameters based on those gradients.\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of each batch is a _loss_ score.\n",
    "\n",
    "It's a measure of how badly the model did on that batch. The larger the score, the worse the model is doing. There is a further note on loss below.\n",
    "\n",
    "Don't worry too much about the exact number. The key thing is that, as we train multiple batches, the loss score should decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3027896881103516"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = train_batch(model, batch, optimizer, loss_function)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_dl, optimizer, loss_function):\n",
    "    \"\"\"\n",
    "    Trains the model for a single epoch.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to train.\n",
    "        train_dl (DataLoader): The DataLoader containing the training data.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer to use for training.\n",
    "        loss_function (nn.Module): The loss function to use for training.\n",
    "\n",
    "    Returns:\n",
    "        float: The average loss value for the epoch.\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    num_batches = len(train_dl)\n",
    "    running_loss = 0\n",
    "    print_every = 10\n",
    "    model.train() # set model to training mode\n",
    "\n",
    "    for i, batch in enumerate(train_dl):\n",
    "\n",
    "        # This next line is all that we need to run through the train set once.\n",
    "        batch_loss = train_batch(model, batch, optimizer, loss_function)\n",
    "\n",
    "        # We'll keep track of the total loss so that we can calculate the average loss at the end.\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        # Every so often, we'll print the average loss so far.\n",
    "        # Note, the actual value of the loss won't mean very much to you.\n",
    "        # The main thing is that we should see it decreasing steadily as we train.\n",
    "        running_loss += batch_loss\n",
    "        if (i+1) % print_every == 0:\n",
    "            print(f'Batch {i+1} - Running loss: {running_loss/print_every:.3f}')\n",
    "            running_loss = 0\n",
    "\n",
    "    return total_loss / num_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have time to run this, we can skip the training by loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_location = torch.device('cpu') if GPU_AVAILABLE else torch.device('cuda')\n",
    "model.load_state_dict(torch.load('../saved_model/model.pt', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do have time to run the model, here's how you do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "Batch 10 - Running loss: 2.265\n",
      "Batch 20 - Running loss: 2.082\n",
      "Batch 30 - Running loss: 1.916\n",
      "Batch 40 - Running loss: 1.816\n",
      "Batch 50 - Running loss: 1.789\n",
      "Batch 60 - Running loss: 1.793\n",
      "Batch 70 - Running loss: 1.758\n",
      "Batch 80 - Running loss: 1.692\n",
      "Batch 90 - Running loss: 1.683\n",
      "Batch 100 - Running loss: 1.661\n",
      "Batch 110 - Running loss: 1.661\n",
      "Batch 120 - Running loss: 1.618\n",
      "Batch 130 - Running loss: 1.607\n",
      "Batch 140 - Running loss: 1.574\n",
      "Batch 150 - Running loss: 1.567\n",
      "Batch 160 - Running loss: 1.568\n",
      "Batch 170 - Running loss: 1.567\n",
      "Batch 180 - Running loss: 1.550\n",
      "Batch 190 - Running loss: 1.536\n",
      "Batch 200 - Running loss: 1.531\n",
      "Batch 210 - Running loss: 1.524\n",
      "Batch 220 - Running loss: 1.534\n",
      "Batch 230 - Running loss: 1.529\n",
      "Batch 240 - Running loss: 1.521\n",
      "Batch 250 - Running loss: 1.525\n",
      "Batch 260 - Running loss: 1.519\n",
      "Batch 270 - Running loss: 1.515\n",
      "Batch 280 - Running loss: 1.516\n",
      "Batch 290 - Running loss: 1.520\n",
      "Batch 300 - Running loss: 1.518\n",
      "Batch 310 - Running loss: 1.513\n",
      "Batch 320 - Running loss: 1.514\n",
      "Batch 330 - Running loss: 1.512\n",
      "Batch 340 - Running loss: 1.508\n",
      "Batch 350 - Running loss: 1.508\n",
      "Batch 360 - Running loss: 1.524\n",
      "Batch 370 - Running loss: 1.502\n",
      "Batch 380 - Running loss: 1.516\n",
      "Batch 390 - Running loss: 1.510\n",
      "Batch 400 - Running loss: 1.508\n",
      "Batch 410 - Running loss: 1.511\n",
      "Batch 420 - Running loss: 1.497\n",
      "Batch 430 - Running loss: 1.504\n",
      "Batch 440 - Running loss: 1.501\n",
      "Batch 450 - Running loss: 1.512\n",
      "Batch 460 - Running loss: 1.516\n",
      "Batch 470 - Running loss: 1.510\n",
      "Batch 480 - Running loss: 1.503\n",
      "Batch 490 - Running loss: 1.517\n",
      "Batch 500 - Running loss: 1.513\n",
      "Batch 510 - Running loss: 1.506\n",
      "Batch 520 - Running loss: 1.500\n",
      "Batch 530 - Running loss: 1.498\n",
      "Batch 540 - Running loss: 1.510\n",
      "Batch 550 - Running loss: 1.497\n",
      "Batch 560 - Running loss: 1.504\n",
      "Batch 570 - Running loss: 1.500\n",
      "Batch 580 - Running loss: 1.503\n",
      "Batch 590 - Running loss: 1.499\n",
      "Batch 600 - Running loss: 1.493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.5776780200004579"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_dl))\n",
    "train_epoch(model, train_dl, optimizer, loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_dl):\n",
    "    \"\"\"\n",
    "    Tests the model on the test data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to test.\n",
    "        test_dl (DataLoader): The DataLoader containing the test data.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy of the model on the test data.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad(): # We don't need to calculate gradients, so we can turn them off to save memory and computation\n",
    "        for batch in test_dl:\n",
    "            x, y = batch['x'], batch['Y']\n",
    "            if GPU_AVAILABLE:\n",
    "                x, y = x.cuda(), y.cuda()\n",
    "            outputs = model(x)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line by line explanation of the test_model function\n",
    "\n",
    "The `test_model` function is used to evaluate the performance of a trained PyTorch model on a test dataset. Here's a breakdown of what each line does:\n",
    "\n",
    "1. `def test_model(model, test_dl):` This line defines the function `test_model` which takes two arguments: `model` (the trained PyTorch model) and `test_dl` (a DataLoader object that loads the test data).\n",
    "\n",
    "2. `correct = 0` and `total = 0`: These lines initialize two counters, `correct` and `total`, to 0. `correct` will keep track of the number of predictions the model gets right, and `total` will keep track of the total number of predictions.\n",
    "\n",
    "3. `model.eval()`: This line sets the model to evaluation mode. This is necessary because some types of layers, like dropout and batch normalization, behave differently during training and evaluation.\n",
    "\n",
    "4. `with torch.no_grad():` This line tells PyTorch not to build the computation graph during the following block of code. This is because we don't need to compute gradients during evaluation, which saves memory and computation.\n",
    "\n",
    "5. `for batch in test_dl:` This line starts a loop over the batches of test data.\n",
    "\n",
    "6. `x, y = batch['x'], batch['Y']`: This line unpacks the input data (`x`) and labels (`y`) from the current batch.\n",
    "\n",
    "7. `if GPU_AVAILABLE: x, y = x.cuda(), y.cuda()`: If a GPU is available, this line moves the input data and labels to the GPU.\n",
    "\n",
    "8. `outputs = model(x)`: This line passes the input data through the model, which computes the forward pass and returns the output.\n",
    "\n",
    "9. `_, predicted = torch.max(outputs.data, 1)`: This line finds the index of the maximum value in the output, which corresponds to the model's prediction.\n",
    "\n",
    "10. `total += y.size(0)`: This line increments the total count by the number of labels in the current batch.\n",
    "\n",
    "11. `correct += (predicted == y).sum().item()`: This line increments the correct count by the number of correct predictions in the current batch.\n",
    "\n",
    "12. `accuracy = 100 * correct / total`: This line calculates the accuracy of the model on the test data as the percentage of correct predictions.\n",
    "\n",
    "13. `return accuracy`: This line returns the calculated accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.44"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(model, test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, we would train for a number of epochs and check our test accuracy after each one. The code would look like this:\n",
    "```\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch:', epoch)\n",
    "    train_epoch(model, train_dl, optimizer, loss_function)\n",
    "    test_model(model, test_dl)\n",
    "```\n",
    "\n",
    "If you want to run your model for multiple epochs, paste this code into a new cell and run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../saved_model/model.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at some examples\n",
    "\n",
    "The code below is not necessary to train the model. However, it gives us the chance to see how the model does predicting different items in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_prediction(model, dataset, index=None):\n",
    "    \"\"\"\n",
    "    Displays a single digit from the test dataset and the model's prediction for it.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to use for prediction.\n",
    "        dataset (Dataset): The PyTorch dataset to use for prediction.\n",
    "        index (int, optional): The index of the datapoint to display. If not provided, a random datapoint will be selected.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # If no index has been supplied, select a random datapoint from the dataset\n",
    "    idx = index if index is not None else np.random.randint(len(dataset))\n",
    "    datapoint = dataset[idx]\n",
    "\n",
    "    # Get the model's prediction for the datapoint\n",
    "    x, y = datapoint['x'], datapoint['Y']\n",
    "    if GPU_AVAILABLE:\n",
    "        x = x.cuda()\n",
    "    confidence_scores = model(x.unsqueeze(0))\n",
    "    predicted_class = torch.argmax(confidence_scores, dim=1).item()\n",
    "\n",
    "    # Display the digit and the model's prediction\n",
    "    plt.imshow(x.squeeze().cpu(), cmap='gray')\n",
    "    plt.title(f'Index: {idx}, True: {y.item()}, Predicted: {predicted_class}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display a random prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_prediction(model, test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find an item that the model got wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_incorrect_prediction(model: nn.Module, dataset: Dataset, num_predictions: int = 10):\n",
    "    \"\"\"\n",
    "    Finds a specified number of incorrect predictions made by a PyTorch model on a dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to use for prediction.\n",
    "        dataset (Dataset): The PyTorch dataset to use for prediction.\n",
    "        num_predictions (int): The number of incorrect predictions to find.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[int, int]]: A list of tuples containing the index of each incorrect prediction and the model's predicted class for that prediction.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the incorrect predictions\n",
    "    incorrect_predictions = []\n",
    "    \n",
    "    # Set the model to evaluation mode so that it is not learning. This will also turn off dropout.\n",
    "    model.eval()\n",
    "    \n",
    "    # Turn off gradient calculation to save memory and computation\n",
    "    with torch.no_grad():\n",
    "        for i, datapoint in enumerate(dataset):\n",
    "\n",
    "            # Get the input data and label for the datapoint\n",
    "            x, y = datapoint['x'], datapoint['Y']\n",
    "            # Move the input data and label to the GPU if available\n",
    "            if GPU_AVAILABLE:\n",
    "                x, y = x.cuda(), y.cuda()\n",
    "            \n",
    "            # Pass the input data through the model to get the predicted class\n",
    "            confidence_scores = model(x.unsqueeze(0))\n",
    "            predicted_class = torch.argmax(confidence_scores, dim=1).item()\n",
    "            \n",
    "            # If the predicted class is not equal to the true label, add the datapoint index and predicted class to the list of incorrect predictions\n",
    "            if predicted_class != y.item():\n",
    "                incorrect_predictions.append((i, y.item(), predicted_class))\n",
    "                \n",
    "                # If we have found the specified number of incorrect predictions, break out of the loop\n",
    "                if len(incorrect_predictions) == num_predictions:\n",
    "                    break\n",
    "\n",
    "    # Return the list of incorrect predictions\n",
    "    return incorrect_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(78, 9, 8),\n",
       " (151, 9, 8),\n",
       " (241, 9, 8),\n",
       " (247, 4, 2),\n",
       " (259, 6, 0),\n",
       " (264, 9, 8),\n",
       " (290, 8, 4),\n",
       " (320, 9, 8),\n",
       " (321, 2, 7),\n",
       " (340, 5, 3)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_incorrect_prediction(model, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_prediction(model, test_ds, 78)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A further note on loss\n",
    "\n",
    "Ideally, the model would produce a confidence score of 1 for the correct answer and zero for all of the others. So, if the correct answer is _2_, the model should output:\n",
    "\n",
    "[0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "(Note that the first digit is the confidence score for zero, which is why the third digit actually corresponds the confidence for the number 2.)\n",
    "\n",
    "In reality, it might produce something like this:\n",
    "\n",
    "[0.01, 0.02, **0.91**, 0.0, 0.01, 0.005, 0.003, 0.02, 0.002, 0.02]\n",
    "\n",
    "Note: it still got the right answer - the highest confidence score is for the number 2. However, even when it gets the right answer, unless it does it perfectly, i.e. with a confidence of 1 for the correct answer and zero for the rest, there will still be a loss.\n",
    "\n",
    "In this way, the model learns to be more confident about the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
